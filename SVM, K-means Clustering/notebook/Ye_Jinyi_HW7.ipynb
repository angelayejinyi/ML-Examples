{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Ye_Jinyi_HW7</h1></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Jinyi Ye\n",
    "<br>\n",
    "Github Username: angelayejinyi\n",
    "<br>\n",
    "USC ID: 1552624974\n",
    "<br>\n",
    "Late Days: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-class and Multi-Label Classification Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download the Anuran Calls (MFCCs) Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('../data/Frogs_MFCCs.csv')\n",
    "\n",
    "# encode the labels in the dataset\n",
    "df = df.iloc[:,:-1]\n",
    "cols = ['Family', 'Genus', 'Species']\n",
    "le = LabelEncoder()\n",
    "for col in cols:\n",
    "    le.fit(df[col])\n",
    "    col_encoded = le.transform(df[col])\n",
    "    df[col] = col_encoded\n",
    "\n",
    "# select 70% randomly as training set\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Train a classifier for each label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exact match (also known as subset accuracy)</b> evaluates the percentage of samples in which all the predicted labels exactly match the true labels. A prediction is only considered correct if it matches all the labels in the ground truth set. The formula for exact match is: \n",
    "\n",
    "$$\n",
    "\\text{exact match} = (\\text{number of samples with all labels correctly predicted}) / (\\text{total number of samples})\n",
    "$$\n",
    "\n",
    "<b>Hamming score/loss</b> is a metric that evaluates the average number of labels that are incorrectly predicted per sample. The Hamming score is the complement of the Hamming loss. Hamming score is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{hamming score} = (\\text{number of correctly predicted labels}) / (\\text{total number of labels})\n",
    "$$\n",
    "\n",
    "And the Hamming loss is defined as:\n",
    "\n",
    "$$\n",
    "\\text{hamming loss} = (\\text{number of incorrectly predicted labels}) / (\\text{total number of labels})\n",
    "$$\n",
    "\n",
    "In both cases, a higher value indicates better performance. Exact match is more suitable when the dataset is highly imbalanced, while Hamming score/loss is more appropriate when the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Train a SVM for each of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, make_scorer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we experiment with combinations of very small and very large λ’s and σ’s and assess the accuracy, in order to get a set of λ’s and σ’s for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the label Family :\n",
      "Accuracy score for first model: 0.992\n",
      "Accuracy score for second model: 0.926\n",
      "For the label Genus :\n",
      "Accuracy score for first model: 0.988\n",
      "Accuracy score for second model: 0.884\n",
      "For the label Species :\n",
      "Accuracy score for first model: 0.990\n",
      "Accuracy score for second model: 0.890\n"
     ]
    }
   ],
   "source": [
    "for col in cols:\n",
    "    X_train = train_df.drop(cols, axis=1)\n",
    "    y_train = train_df[col]\n",
    "    X_test = test_df.drop(cols, axis=1)\n",
    "    y_test = test_df[col]\n",
    "\n",
    "    print('For the label', col, ':')\n",
    "\n",
    "    svm_try1 = SVC(kernel='rbf', C=1000000, gamma=0.1) # large penalty, simple decision boundary\n",
    "    ovr_try1 = OneVsRestClassifier(svm_try1)\n",
    "    ovr_try1.fit(X_train, y_train)\n",
    "    y_pred = ovr_try1.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy score for first model: {:.3f}\".format(acc))\n",
    "\n",
    "    svm_try2 = SVC(kernel='rbf', C=0.01, gamma=2) # small penalty, complex decision boundary\n",
    "    ovr_try2 = OneVsRestClassifier(svm_try2)\n",
    "    ovr_try2.fit(X_train, y_train)\n",
    "    y_pred = ovr_try2.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy score for second model: {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above analysis shows, the accuracy of a support vector machine will not be below 70% for λ = 0.01 and λ = 1000000, we can choose log(λ) ∈ {−2,...,4,5,6}. For the Gaussian Kernel parameter, we can choose linear increments σ ∈ {.2, .4, . . . , 2}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(cols, axis=1)\n",
    "y_train = train_df[cols]\n",
    "X_test = test_df.drop(cols, axis=1)\n",
    "y_test = test_df[cols]\n",
    "\n",
    "param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000],\n",
    "        'gamma': [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]\n",
    "    }\n",
    "\n",
    "def svm_gaussian(X_train, y_train, param_grid):\n",
    "    \n",
    "    # initialize and train the SVM classifiers for each label\n",
    "    svm_classifiers = [SVC(kernel='rbf', C=1, gamma=1, decision_function_shape='ovr') for _ in range(y_train.shape[1])]\n",
    "\n",
    "    for i, clf in enumerate(svm_classifiers):\n",
    "        print(f'Training SVM for label {cols[i]}')\n",
    "        y_label_train = y_train.iloc[:, i]\n",
    "        scorer = make_scorer(hamming_loss, greater_is_better=False)\n",
    "        grid_search = GridSearchCV(clf, param_grid, scoring=scorer, cv=10, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_label_train)\n",
    "        print(f'Best parameters: {grid_search.best_params_}')\n",
    "        svm_classifiers[i] = grid_search.best_estimator_\n",
    "    \n",
    "    return svm_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try with unstandardized features. We report exact match and hamming loss of the prediction of three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM for label Family\n",
      "Best parameters: {'C': 10, 'gamma': 2.0}\n",
      "Training SVM for label Genus\n",
      "Best parameters: {'C': 100, 'gamma': 1.6}\n",
      "Training SVM for label Species\n",
      "Best parameters: {'C': 10, 'gamma': 1.8}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SVC(C=10, gamma=2.0), SVC(C=100, gamma=1.6), SVC(C=10, gamma=1.8)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best parameters\n",
    "svm_gaussian(X_train, y_train, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy - Exact match: 0.99980, Hamming loss: 0.99987\n",
      "Test set accuracy - Exact match: 0.98749, Hamming loss: 0.99166\n"
     ]
    }
   ],
   "source": [
    "# fit data, predict and calculate loss\n",
    "svm_classifiers = [SVC(kernel='rbf', C=10, gamma=2.0, decision_function_shape='ovr'),\n",
    "                   SVC(kernel='rbf', C=100, gamma=1.6, decision_function_shape='ovr'),\n",
    "                   SVC(kernel='rbf', C=10, gamma=1.8, decision_function_shape='ovr')]\n",
    "\n",
    "# define a function to calculate loss\n",
    "def svm_performance(svm_classifiers, X_train, X_test):\n",
    "    y_train_pred = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for i, clf in enumerate(svm_classifiers):\n",
    "        y_label_train = y_train.iloc[:, i]\n",
    "        clf.fit(X_train, y_label_train)\n",
    "        y_train_pred.append(clf.predict(X_train))\n",
    "        y_test_pred.append(clf.predict(X_test))\n",
    "\n",
    "    y_train_pred = np.array(y_train_pred).T\n",
    "    y_test_pred = np.array(y_test_pred).T\n",
    "\n",
    "    exact_match_train = np.all(y_train == y_train_pred, axis=1).mean()\n",
    "    hamming_loss_train = np.mean(np.not_equal(np.array(y_train), y_train_pred))\n",
    "    print(\"Train set accuracy - Exact match: {:.5f}, Hamming loss: {:.5f}\".format(exact_match_train, 1 - hamming_loss_train))\n",
    "\n",
    "    exact_match_test = np.all(y_test == y_test_pred, axis=1).mean()\n",
    "    hamming_loss_test = np.mean(np.not_equal(np.array(y_test), y_test_pred))\n",
    "    print(\"Test set accuracy - Exact match: {:.5f}, Hamming loss: {:.5f}\".format(exact_match_test, 1 - hamming_loss_test))\n",
    "\n",
    "svm_performance(svm_classifiers, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try with standardized features. We report exact match and hamming loss of the prediction of three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_standardized = scaler.transform(X_train)\n",
    "X_train_standardized = pd.DataFrame(X_train_standardized, columns=X_train.columns)\n",
    "\n",
    "scaler.fit(X_test)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "X_test_standardized = pd.DataFrame(X_test_standardized, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM for label Family\n",
      "Best parameters: {'C': 10, 'gamma': 0.2}\n",
      "Training SVM for label Genus\n",
      "Best parameters: {'C': 10, 'gamma': 0.2}\n",
      "Training SVM for label Species\n",
      "Best parameters: {'C': 10, 'gamma': 0.2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SVC(C=10, gamma=0.2), SVC(C=10, gamma=0.2), SVC(C=10, gamma=0.2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best parameters\n",
    "param_grid = {\n",
    "        'C': [1, 10, 100, 1000, 10000, 100000, 1000000],\n",
    "        'gamma': [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]\n",
    "    }\n",
    "\n",
    "svm_gaussian(X_train_standardized, y_train, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy - Exact match: 1.00000, Hamming loss: 1.00000\n",
      "Test set accuracy - Exact match: 0.97175, Hamming loss: 0.97993\n"
     ]
    }
   ],
   "source": [
    "# fit data, predict and calculate loss\n",
    "svm_classifiers2 = [SVC(kernel='rbf', C=10, gamma=0.2, decision_function_shape='ovr'),\n",
    "                   SVC(kernel='rbf', C=10, gamma=0.2, decision_function_shape='ovr'),\n",
    "                   SVC(kernel='rbf', C=10, gamma=0.2, decision_function_shape='ovr')]\n",
    "\n",
    "svm_performance(svm_classifiers2, X_train_standardized, X_test_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can observe that the model performs better on test data with non-standardized features for both exact match and hamming loss metrics. The reason may be that if the data is already normalized, standardizing the data can introduce additional noise or variability. This can also cause the model to overfit the training data with the noise, as can be seen in the 1.00000 train set accuracy for the model using standardized features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Repeat 1(b)ii with L1-penalized SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def svm_linear(X_train, y_train, param_grid):\n",
    "    \n",
    "    # initialize and train the SVM classifiers for each label\n",
    "    svm_classifiers = [OneVsRestClassifier(LinearSVC(penalty='l1', dual=False, random_state=42, max_iter=50000)) for _ in range(y_train.shape[1])]\n",
    "\n",
    "    for i, clf in enumerate(svm_classifiers):\n",
    "        print(f'Training SVM for label {cols[i]}')\n",
    "        y_label_train = y_train.iloc[:, i]\n",
    "        scorer = make_scorer(hamming_loss, greater_is_better=False)\n",
    "        grid_search = GridSearchCV(clf, param_grid, scoring=scorer, cv=10, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_label_train)\n",
    "        print(f'Best parameters: {grid_search.best_params_}')\n",
    "        svm_classifiers[i] = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM for label Family\n",
      "Best parameters: {'estimator__C': 1}\n",
      "Training SVM for label Genus\n",
      "Best parameters: {'estimator__C': 10}\n",
      "Training SVM for label Species\n",
      "Best parameters: {'estimator__C': 10}\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters\n",
    "param_grid = {\n",
    "        'estimator__C': [0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "    }\n",
    "\n",
    "svm_linear(X_train_standardized, y_train, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy - Exact match: 0.92454, Hamming loss: 0.95453\n",
      "Test set accuracy - Exact match: 0.90922, Hamming loss: 0.94133\n"
     ]
    }
   ],
   "source": [
    "svm_classifiers3 = [OneVsRestClassifier(LinearSVC(C=1, penalty='l1', dual=False, random_state=42, max_iter=50000)),\n",
    "                   OneVsRestClassifier(LinearSVC(C=10, penalty='l1', dual=False, random_state=42, max_iter=50000)),\n",
    "                   OneVsRestClassifier(LinearSVC(C=10, penalty='l1', dual=False, random_state=42, max_iter=50000))]\n",
    "\n",
    "svm_performance(svm_classifiers3, X_train_standardized, X_test_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for this dataset, L1 penalty and linear kernel result in lower classification accuracy compared to the Gaussian kernel. The reason can be: 1) The linear kernel only allows for linear decision boundaries, while the Gaussian kernel can model non-linear decision boundaries more effectively, the data in this dataset might not be linearly separable, 2) While L1 penalty encourages sparsity in the feature weights, it may also remove some relevant features along with the irrelevant ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) Repeat 1(b)iii by using SMOTE or any other method for imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# write a function using SMOTE on train data and evaluate performance\n",
    "def svm_linear_smote(X_train, y_train, X_test, y_test, param_grid):\n",
    "    \n",
    "    # initialize and train the SVM classifiers for each label\n",
    "    svm_classifiers = [OneVsRestClassifier(LinearSVC(penalty='l1', dual=False, random_state=42, max_iter=50000)) for _ in range(y_train.shape[1])]\n",
    "\n",
    "    y_test_pred = []\n",
    "    hamming_loss_score = []\n",
    "    \n",
    "    for i, clf in enumerate(svm_classifiers):\n",
    "        print(f'Training SVM for label {cols[i]}')\n",
    "        y_label_train = y_train.iloc[:, i]\n",
    "        # Apply SMOTE to the train data\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_label_train)\n",
    "        scorer = make_scorer(hamming_loss, greater_is_better=False)\n",
    "        grid_search = GridSearchCV(clf, param_grid, scoring=scorer, cv=10, n_jobs=-1)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        print(f'Best parameters: {grid_search.best_params_}')\n",
    "        svm_classifiers[i] = grid_search.best_estimator_\n",
    "        \n",
    "        final_model = OneVsRestClassifier(LinearSVC(penalty='l1', C=grid_search.best_params_['estimator__C'],\n",
    "                                                   dual=False, random_state=42, max_iter=50000))\n",
    "        final_model.fit(X_train_resampled, y_train_resampled)\n",
    "        y_train_pred = final_model.predict(X_train_resampled)\n",
    "        y_test_pred.append(final_model.predict(X_test))\n",
    "        hamming_loss_score.append(hamming_loss(y_train_resampled, np.array(y_train_pred).T))\n",
    "    \n",
    "    y_test_pred = np.array(y_test_pred).T\n",
    "    \n",
    "    hamming_loss_train = np.mean(hamming_loss_score)\n",
    "    print(\"Train set accuracy - Hamming loss: {:.5f}\".format(1 - hamming_loss_train))\n",
    "\n",
    "    exact_match_test = np.all(y_test == y_test_pred, axis=1).mean()\n",
    "    hamming_loss_test = np.mean(np.not_equal(np.array(y_test), y_test_pred))\n",
    "    print(\"Test set accuracy - Exact match: {:.5f}, Hamming loss: {:.5f}\".format(exact_match_test, 1 - hamming_loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM for label Family\n",
      "Best parameters: {'estimator__C': 100}\n",
      "Training SVM for label Genus\n",
      "Best parameters: {'estimator__C': 100}\n",
      "Training SVM for label Species\n",
      "Best parameters: {'estimator__C': 1000}\n",
      "Train set accuracy - Hamming loss: 0.95947\n",
      "Test set accuracy - Exact match: 0.86614, Hamming loss: 0.92697\n"
     ]
    }
   ],
   "source": [
    "# get the best parameters and model accuracy\n",
    "param_grid = {\n",
    "        'estimator__C': [0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "    }\n",
    "\n",
    "svm_linear_smote(X_train_standardized, y_train, X_test_standardized, y_test, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that test accuracy is lower after applying SMOTE to the training data. This may be due to overfitting to the synthetic samples, which may not be representative of the true distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Means Clustering on a Multi-Class and Multi-Label Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# use whole dataset for X and y\n",
    "X = df.drop(cols, axis=1)\n",
    "y = df[cols]\n",
    "\n",
    "# Define range of k values to try\n",
    "k_values = range(2, 51) \n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    score = silhouette_score(X, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Get the best k\n",
    "chosen_k = np.argmax(silhouette_scores) + 2\n",
    "kmeans = KMeans(n_clusters=chosen_k, random_state=42)\n",
    "kmeans.fit(X)\n",
    "print(f'Best K: {chosen_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the silhouette score as a function of k\n",
    "plt.plot(k_values, silhouette_scores)\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette score plot for k-means clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Determine which family/genus/species is the majority in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique labels (families, genus, and species)\n",
    "unique_families = np.unique(df['Family'])\n",
    "unique_genus = np.unique(df['Genus'])\n",
    "unique_species = np.unique(df['Species'])\n",
    "\n",
    "# Iterate over each cluster and determine the majority label for each level\n",
    "majority_families_lst = []\n",
    "majority_genus_lst = []\n",
    "majority_species_lst = []\n",
    "\n",
    "for cluster in range(4):\n",
    "    # Get the indices of samples in this cluster\n",
    "    cluster_indices = np.where(kmeans.labels_ == cluster)[0]\n",
    "    \n",
    "    # Get the true labels of the samples in this cluster\n",
    "    cluster_labels = y.iloc[cluster_indices]\n",
    "    \n",
    "    # Count the occurrences of each label at the family level\n",
    "    family_counts = np.zeros(len(unique_families))\n",
    "    for i, family in enumerate(unique_families):\n",
    "        family_counts[i] = np.sum(cluster_labels['Family'] == family)\n",
    "    majority_family = unique_families[np.argmax(family_counts)]\n",
    "    majority_families_lst.append(majority_family)\n",
    "    \n",
    "    # Count the occurrences of each label at the genus level\n",
    "    genus_counts = np.zeros(len(unique_genus))\n",
    "    for i, genus in enumerate(unique_genus):\n",
    "        genus_counts[i] = np.sum(cluster_labels['Genus'] == genus)\n",
    "    majority_genus = unique_genus[np.argmax(genus_counts)]\n",
    "    majority_genus_lst.append(majority_genus)\n",
    "    \n",
    "    # Count the occurrences of each label at the species level\n",
    "    species_counts = np.zeros(len(unique_species))\n",
    "    for i, species in enumerate(unique_species):\n",
    "        species_counts[i] = np.sum(cluster_labels['Species'] == species)\n",
    "    majority_species = unique_species[np.argmax(species_counts)]\n",
    "    majority_species_lst.append(majority_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cluster 1:\n",
      "The majority family is 3\n",
      "The majority genus is 0\n",
      "The majority species is 1\n",
      "For Cluster 2:\n",
      "The majority family is 2\n",
      "The majority genus is 3\n",
      "The majority species is 5\n",
      "For Cluster 3:\n",
      "The majority family is 2\n",
      "The majority genus is 3\n",
      "The majority species is 4\n",
      "For Cluster 4:\n",
      "The majority family is 1\n",
      "The majority genus is 1\n",
      "The majority species is 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    print(f'For Cluster {i}:')\n",
    "    print(f'The majority family is {majority_families_lst[i-1]}')\n",
    "    print(f'The majority genus is {majority_genus_lst[i-1]}')\n",
    "    print(f'The majority species is {majority_species_lst[i-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Monte-Carlo Simulation (50 iterations), calculate the average Hamming distance, Hamming score, and Hamming loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamming distance is the average number of positions in which two label sets differ.\n",
    "Hamming score is the ratio of the number of correctly classified labels to the total number of labels.\n",
    "Hamming loss is the fraction of labels that are incorrectly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we write a function to combine (a)-(c) together\n",
    "\n",
    "def kmeans_clustering(X,y):\n",
    "    \n",
    "    # Get the best k\n",
    "    k_values = range(2, 51) \n",
    "    silhouette_scores = []\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(X)\n",
    "        score = silhouette_score(X, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "    chosen_k = np.argmax(silhouette_scores) + 2\n",
    "    kmeans = KMeans(n_clusters=chosen_k)\n",
    "    kmeans.fit(X)\n",
    "    print(f'Best K: {chosen_k}')\n",
    "    \n",
    "    # Determine which family/genus/species is the majority in each cluster\n",
    "    unique_families = np.unique(y['Family'])\n",
    "    unique_genus = np.unique(y['Genus'])\n",
    "    unique_species = np.unique(y['Species'])\n",
    "\n",
    "    majority_families_lst = []\n",
    "    majority_genus_lst = []\n",
    "    majority_species_lst = []\n",
    "\n",
    "    for cluster in range(chosen_k):\n",
    "        cluster_indices = np.where(kmeans.labels_ == cluster)[0]\n",
    "        cluster_labels = y.iloc[cluster_indices]\n",
    "\n",
    "        # Count the occurrences of each label at the family level\n",
    "        family_counts = np.zeros(len(unique_families))\n",
    "        for i, family in enumerate(unique_families):\n",
    "            family_counts[i] = np.sum(cluster_labels['Family'] == family)\n",
    "        majority_family = unique_families[np.argmax(family_counts)]\n",
    "        majority_families_lst.append(majority_family)\n",
    "\n",
    "        # Count the occurrences of each label at the genus level\n",
    "        genus_counts = np.zeros(len(unique_genus))\n",
    "        for i, genus in enumerate(unique_genus):\n",
    "            genus_counts[i] = np.sum(cluster_labels['Genus'] == genus)\n",
    "        majority_genus = unique_genus[np.argmax(genus_counts)]\n",
    "        majority_genus_lst.append(majority_genus)\n",
    "\n",
    "        # Count the occurrences of each label at the species level\n",
    "        species_counts = np.zeros(len(unique_species))\n",
    "        for i, species in enumerate(unique_species):\n",
    "            species_counts[i] = np.sum(cluster_labels['Species'] == species)\n",
    "        majority_species = unique_species[np.argmax(species_counts)]\n",
    "        majority_species_lst.append(majority_species)\n",
    "\n",
    "    \n",
    "    # Calculate the average Hamming distance, Hamming score, and Hamming loss\n",
    "    y_pred = [np.zeros(len(y)), np.zeros(len(y)), np.zeros(len(y))]\n",
    "\n",
    "    for i in range(4):\n",
    "        # Get the indices of samples in this cluster\n",
    "        cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "        y_pred[0][cluster_indices] = majority_families_lst[i]\n",
    "        y_pred[1][cluster_indices] = majority_genus_lst[i]\n",
    "        y_pred[2][cluster_indices] = majority_species_lst[i]\n",
    "\n",
    "    y_pred = np.array(y_pred).T\n",
    "    hamming_distance = np.sum(np.sum(y != y_pred)) / len(y)\n",
    "    hamming_loss = np.mean(np.not_equal(np.array(y), y_pred))\n",
    "    hamming_score = 1 - hamming_loss\n",
    "    \n",
    "    return hamming_distance, hamming_loss, hamming_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Monte Carlo function:\n",
    "def monteCarlo(times, X, y):\n",
    "    \n",
    "    hamming_distance_lst = []\n",
    "    hamming_loss_lst = []\n",
    "    hamming_score_lst = []\n",
    "    \n",
    "    for i in range(times):\n",
    "        hamming_distance, hamming_loss, hamming_score = kmeans_clustering(X, y)\n",
    "        hamming_distance_lst.append(hamming_distance)\n",
    "        hamming_loss_lst.append(hamming_loss)\n",
    "        hamming_score_lst.append(hamming_score)\n",
    "        print(\"Attempt {} ~ Hamming Distance : {}, Hamming Loss : {}, Hamming Score : {}\".format(i+1, hamming_distance, hamming_loss, hamming_score))\n",
    "    \n",
    "    return hamming_distance_lst, hamming_loss_lst, hamming_score_lst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to computational cost, the 50 iterations are executed in 5 times, with 10 iterations each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: 4\n",
      "Attempt 1 ~ Hamming Distance : 0.6668519805420431, Hamming Loss : 0.22228399351401437, Hamming Score : 0.7777160064859856\n",
      "Best K: 4\n",
      "Attempt 2 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 3 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 4 ~ Hamming Distance : 0.7357887421820709, Hamming Loss : 0.24526291406069028, Hamming Score : 0.7547370859393097\n",
      "Best K: 4\n",
      "Attempt 5 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 6 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 7 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 8 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 9 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 10 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n"
     ]
    }
   ],
   "source": [
    "hamming_distance_lst, hamming_loss_lst, hamming_score_lst = monteCarlo(10, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: 4\n",
      "Attempt 1 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 2 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 3 ~ Hamming Distance : 0.7009034051424601, Hamming Loss : 0.23363446838082003, Hamming Score : 0.76636553161918\n",
      "Best K: 4\n",
      "Attempt 4 ~ Hamming Distance : 0.6669909659485754, Hamming Loss : 0.22233032198285846, Hamming Score : 0.7776696780171415\n",
      "Best K: 4\n",
      "Attempt 5 ~ Hamming Distance : 0.6674079221681724, Hamming Loss : 0.22246930738939077, Hamming Score : 0.7775306926106093\n",
      "Best K: 4\n",
      "Attempt 6 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 7 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 8 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 9 ~ Hamming Distance : 0.6668519805420431, Hamming Loss : 0.22228399351401437, Hamming Score : 0.7777160064859856\n",
      "Best K: 4\n",
      "Attempt 10 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n"
     ]
    }
   ],
   "source": [
    "hamming_distance_lst2, hamming_loss_lst2, hamming_score_lst2 = monteCarlo(10, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: 4\n",
      "Attempt 1 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 2 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 3 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 4 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 5 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 6 ~ Hamming Distance : 0.6668519805420431, Hamming Loss : 0.22228399351401437, Hamming Score : 0.7777160064859856\n",
      "Best K: 4\n",
      "Attempt 7 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 8 ~ Hamming Distance : 0.735371785962474, Hamming Loss : 0.24512392865415797, Hamming Score : 0.754876071345842\n",
      "Best K: 4\n",
      "Attempt 9 ~ Hamming Distance : 0.6664350243224462, Hamming Loss : 0.22214500810748206, Hamming Score : 0.7778549918925179\n",
      "Best K: 4\n",
      "Attempt 10 ~ Hamming Distance : 0.6674079221681724, Hamming Loss : 0.22246930738939077, Hamming Score : 0.7775306926106093\n"
     ]
    }
   ],
   "source": [
    "hamming_distance_lst3, hamming_loss_lst3, hamming_score_lst3 = monteCarlo(10, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: 4\n",
      "Attempt 1 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 2 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 3 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 4 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 5 ~ Hamming Distance : 0.6657400972897846, Hamming Loss : 0.2219133657632615, Hamming Score : 0.7780866342367385\n",
      "Best K: 4\n",
      "Attempt 6 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 7 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 8 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 9 ~ Hamming Distance : 0.6653231410701876, Hamming Loss : 0.22177438035672922, Hamming Score : 0.7782256196432707\n",
      "Best K: 4\n",
      "Attempt 10 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n"
     ]
    }
   ],
   "source": [
    "hamming_distance_lst4, hamming_loss_lst4, hamming_score_lst4 = monteCarlo(10, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: 4\n",
      "Attempt 1 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 2 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 3 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 4 ~ Hamming Distance : 0.6653231410701876, Hamming Loss : 0.22177438035672922, Hamming Score : 0.7782256196432707\n",
      "Best K: 4\n",
      "Attempt 5 ~ Hamming Distance : 0.7357887421820709, Hamming Loss : 0.24526291406069028, Hamming Score : 0.7547370859393097\n",
      "Best K: 4\n",
      "Attempt 6 ~ Hamming Distance : 0.6657400972897846, Hamming Loss : 0.2219133657632615, Hamming Score : 0.7780866342367385\n",
      "Best K: 4\n",
      "Attempt 7 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 8 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 9 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n",
      "Best K: 4\n",
      "Attempt 10 ~ Hamming Distance : 0.66726893676164, Hamming Loss : 0.2224229789205467, Hamming Score : 0.7775770210794533\n"
     ]
    }
   ],
   "source": [
    "hamming_distance_lst5, hamming_loss_lst5, hamming_score_lst5 = monteCarlo(10, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the 5 lists together\n",
    "hamming_distance_lst_all = []\n",
    "hamming_distance_lst_all.extend(hamming_distance_lst)\n",
    "hamming_distance_lst_all.extend(hamming_distance_lst2)\n",
    "hamming_distance_lst_all.extend(hamming_distance_lst3)\n",
    "hamming_distance_lst_all.extend(hamming_distance_lst4)\n",
    "hamming_distance_lst_all.extend(hamming_distance_lst5)\n",
    "\n",
    "hamming_loss_lst_all = []\n",
    "hamming_loss_lst_all.extend(hamming_loss_lst)\n",
    "hamming_loss_lst_all.extend(hamming_loss_lst2)\n",
    "hamming_loss_lst_all.extend(hamming_loss_lst3)\n",
    "hamming_loss_lst_all.extend(hamming_loss_lst4)\n",
    "hamming_loss_lst_all.extend(hamming_loss_lst5)\n",
    "\n",
    "hamming_score_lst_all = []\n",
    "hamming_score_lst_all.extend(hamming_score_lst)\n",
    "hamming_score_lst_all.extend(hamming_score_lst2)\n",
    "hamming_score_lst_all.extend(hamming_score_lst3)\n",
    "hamming_score_lst_all.extend(hamming_score_lst4)\n",
    "hamming_score_lst_all.extend(hamming_score_lst5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average Hamming Distance is 0.6718637943015981\n",
      "The standard deviation of simulation result is 0.016803063424687598\n",
      "The average Hamming Loss is 0.22395459810053278\n",
      "The standard deviation of simulation result is 0.005601021141562524\n",
      "The average Hamming Score is 0.7760454018994671\n",
      "The standard deviation of simulation result is 0.005601021141562524\n"
     ]
    }
   ],
   "source": [
    "# get the average and standard deviation\n",
    "print(\"The average Hamming Distance is {}\".format(np.mean(hamming_distance_lst_all)))\n",
    "print(\"The standard deviation of simulation result is {}\".format(np.std(hamming_distance_lst_all)))\n",
    "print(\"The average Hamming Loss is {}\".format(np.mean(hamming_loss_lst_all)))\n",
    "print(\"The standard deviation of simulation result is {}\".format(np.std(hamming_loss_lst_all)))\n",
    "print(\"The average Hamming Score is {}\".format(np.mean(hamming_score_lst_all)))\n",
    "print(\"The standard deviation of simulation result is {}\".format(np.std(hamming_score_lst_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ISLR 12.6.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Dendrogram with complete linkage (See dendrogram1.jpg)\n",
    "\n",
    "![title](dendrogram1.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Dendrogram with simple linkage (See dendrogram2.jpg)\n",
    "\n",
    "![title](dendrogram2.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) We get two clusters: {1, 2} and {3, 4}."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) We get two clusters: {1, 2, 3} and {4}."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Dendrogram equivalent to the one in (a) (See dendrogram3.jpg)\n",
    "\n",
    "![title](dendrogram3.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
